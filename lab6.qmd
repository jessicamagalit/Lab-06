---
project:
  type: website
  output-dir: docs
title: "Lab 6 Machine Learning in Hydrology"
author: Jessie Magalit
format:
  html:
    self-contained: true
---

# Lab Set Up
```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(workflows)
library(rsample)
library(recipes)
library(tune)
library(ranger)
library(xgboost)
library(nnet)
```

```{r}
root  <- 'https://gdex.ucar.edu/dataset/camels/file'

download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')

types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
remote_files  <- glue('{root}/camels_{types}.txt')
local_files   <- glue('data/camels_{types}.txt')

walk2(remote_files, local_files, download.file, quiet = TRUE)
camels <- map(local_files, read_delim, show_col_types = FALSE) 

camels <- map(local_files, read_delim, show_col_types = FALSE) 

camels <- power_full_join(camels ,by = 'gauge_id')
```

## Question 1

From the PDF, zero_q_freq represents the frequency of days where Q = 0 mm/day.

```{r}
library(ggplot2)
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  ggthemes::theme_map()
```

## Question 2

```{r}
camels %>%
  select(aridity, p_mean, q_mean) %>% 
  drop_na() %>%
  cor()
```

```{r}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm", color = "red", linetype = 2) +
  scale_color_viridis_c() +
  theme_linedraw() + 
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

```{r}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  scale_color_viridis_c() +
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

```{r}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  scale_color_viridis_c(trans = "log") +
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom",
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(.5, "cm")) + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

```{r}
set.seed(123)

camels <- camels %>%
  mutate(logQmean = log(q_mean))
```

```{r}
camels_split <- initial_split(camels, prop = 0.8)

camels_train <- training(camels_split)

camels_test  <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)
```

```{r}
rec <-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  step_log(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_interact(terms = ~ aridity:p_mean) %>% 
  step_naomit(all_predictors(), all_outcomes())
```

```{r}
baked_data <- prep(rec, camels_train) %>% 
  bake(new_data = NULL)

lm_base <- lm(logQmean ~ aridity * p_mean, data = baked_data)

summary(lm_base)
```

```{r}
summary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))
```

```{r}
test_data <-  bake(prep(rec), new_data = camels_test)

test_data$lm_pred <- predict(lm_base, newdata = test_data)
```

```{r}
metrics(test_data, truth = logQmean, estimate = lm_pred)
```

```{r}
ggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +
  # Apply a gradient color scale
  scale_color_gradient2(low = "brown", mid = "orange", high = "darkgreen") +
  geom_point() +
  geom_abline(linetype = 2) +
  theme_linedraw() + 
  labs(title = "Linear Model: Observed vs Predicted",
       x = "Observed Log Mean Flow",
       y = "Predicted Log Mean Flow",
       color = "Aridity")
```

```{r}
lm_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")
```

```{r}
lm_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(lm_model) %>%
  fit(data = camels_train) 

summary(extract_fit_engine(lm_wf))$coefficients

summary(lm_base)$coefficients
```

```{r}
lm_data <- augment(lm_wf, new_data = camels_test)
dim(lm_data)
```

```{r}
metrics(lm_data, truth = logQmean, estimate = .pred)
```

```{r}
ggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

```{r}
rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(rf_model) %>%
  fit(data = camels_train) 

rf_data <- augment(rf_wf, new_data = camels_test)
dim(rf_data)
```

```{r}
metrics(rf_data, truth = logQmean, estimate = .pred)
```

```{r}
ggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

```{r}
#rec <- recipe(logQmean ~ ., data = camels_train)
```

```{r}
#camels_train$outcome <- factor(camels_train$outcome, levels = c("no", "yes"))
```

```{r}
boost_mod <- boost_tree() %>%
  set_engine('xgboost') %>%
  set_mode("regression")

nn_model <- bag_mlp() %>% 
  set_engine("nnet") %>% 
  set_mode("regression")
```

```{r}
wf <- workflow_set(list(rec), list(lm_model, rf_model, boost_mod, nn_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv) 

autoplot(wf)
```
## Question 3

```{r}
rank_results(wf, rank_metric = "rsq", select_best = TRUE)
```
It seems as if the bagged MLP is the best bet for which model to use, as it has a lower rmse and higher rsq than the other models.

## Build Your Own

## Data Splitting

```{r}
set.seed(123)

my_camels_split <- initial_split(camels, prop = 0.75)

my_camels_train <- training(my_camels_split)

my_camels_test  <- testing(my_camels_split)

my_camels_cv <- vfold_cv(my_camels_train, v = 10)
```

## Recipe

```{r}
my_rec <-  recipe(logQmean ~ baseflow_index + soil_porosity, data = my_camels_train) %>%
  step_log(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_interact(terms = ~ baseflow_index:soil_porosity) %>% 
  step_naomit(all_predictors(), all_outcomes())
```
I'm choosing these predictors because daily water discharge and the porosity of soil sound like they would have an impact on mean water discharge. Soils with greater porosity will likely have lower mean discharges, and areas with higher daily water discharges will likely have higher mean discharges.

## Define 3 Models

```{r}
my_rf <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("regression")

my_lm <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

my_boost <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")
```

## Workflow Set

```{r}
my_wf <- workflow_set(list(my_rec), list(my_lm, my_rf, my_boost)) %>%
  workflow_map('fit_resamples', resamples = my_camels_cv) 
```
## Evaluation

```{r}
autoplot(my_wf)
```

```{r}
rank_results(my_wf, rank_metric = "rsq", select_best = TRUE)
```
The rand forest model fits this the best, as it has the lowest rmse and the highest rsq.

## Extract and Evaluate

```{r}
my_lm_wf <- workflow() %>%
  add_recipe(my_rec) %>%
  add_model(my_lm) %>%
  fit(data = my_camels_train) 

summary(extract_fit_engine(my_lm_wf))$coefficients
```
```{r}
my_lm_data <- augment(my_lm_wf, new_data = my_camels_test)
dim(my_lm_data)
```
```{r}
ggplot(my_lm_data, aes(x = logQmean, y = .pred, colour = baseflow_index)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw() + 
  labs(title = "Linear Model: Observed vs Predicted",
       x = "Observed Log Mean Flow",
       y = "Predicted Log Mean Flow",
       color = "Baseflow Index")


```
To be candid, the results are quite underwhelming. There appears to be minimal correlation between the variables, and the line of best fit does not align well with the scatterplot distribution.

[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(workflows)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(tune)\nlibrary(ranger)\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(nnet)\n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n\n\nFrom the PDF, zero_q_freq represents the frequency of days where Q = 0 mm/day.\n\nlibrary(ggplot2)\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\n\n\n\ncamels %&gt;%\n  select(aridity, p_mean, q_mean) %&gt;% \n  drop_na() %&gt;%\n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  scale_color_viridis_c() +\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\ncamels &lt;- camels %&gt;%\n  mutate(logQmean = log(q_mean))\n\n\ncamels_split &lt;- initial_split(camels, prop = 0.8)\n\ncamels_train &lt;- training(camels_split)\n\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) %&gt;% \n  step_naomit(all_predictors(), all_outcomes())\n\n\nbaked_data &lt;- prep(rec, camels_train) %&gt;% \n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\n\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.08342    0.02898  -2.879  0.00415 ** \naridity        -0.39770    0.06819  -5.832 9.51e-09 ***\np_mean          0.65025    0.06882   9.449  &lt; 2e-16 ***\naridity:p_mean  0.02389    0.01640   1.457  0.14583    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -0.08342    0.02898  -2.879  0.00415 ** \naridity          -0.39770    0.06819  -5.832 9.51e-09 ***\np_mean            0.65025    0.06882   9.449  &lt; 2e-16 ***\naridity_x_p_mean  0.02389    0.01640   1.457  0.14583    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\n\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\nlm_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n\nlm_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(lm_model) %&gt;%\n  fit(data = camels_train) \n\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                    Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)      -0.08341681 0.02897625 -2.878799 4.152841e-03\naridity          -0.39769793 0.06818909 -5.832281 9.507063e-09\np_mean            0.65025318 0.06882050  9.448539 1.088651e-19\naridity_x_p_mean  0.02389458 0.01640487  1.456555 1.458304e-01\n\nsummary(lm_base)$coefficients\n\n                  Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)    -0.08341681 0.02897625 -2.878799 4.152841e-03\naridity        -0.39769793 0.06818909 -5.832281 9.507063e-09\np_mean          0.65025318 0.06882050  9.448539 1.088651e-19\naridity:p_mean  0.02389458 0.01640487  1.456555 1.458304e-01\n\n\n\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\n\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_model) %&gt;%\n  fit(data = camels_train) \n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.594\n2 rsq     standard       0.736\n3 mae     standard       0.369\n\n\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n#rec &lt;- recipe(logQmean ~ ., data = camels_train)\n\n\n#camels_train$outcome &lt;- factor(camels_train$outcome, levels = c(\"no\", \"yes\"))\n\n\nboost_mod &lt;- boost_tree() %&gt;%\n  set_engine('xgboost') %&gt;%\n  set_mode(\"regression\")\n\nnn_model &lt;- bag_mlp() %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"regression\")\n\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, boost_mod, nn_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.538  0.0273    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.791  0.0247    10 recipe       bag_…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n5 recipe_rand_fore… Prepro… rmse    0.567  0.0262    10 recipe       rand…     3\n6 recipe_rand_fore… Prepro… rsq     0.767  0.0238    10 recipe       rand…     3\n7 recipe_boost_tree Prepro… rmse    0.607  0.0283    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.740  0.0288    10 recipe       boos…     4\n\n\nIt seems as if the bagged MLP is the best bet for which model to use, as it has a lower rmse and higher rsq than the other models.\n\n\n\n\n\n\n\nset.seed(123)\n\nmy_camels_split &lt;- initial_split(camels, prop = 0.75)\n\nmy_camels_train &lt;- training(my_camels_split)\n\nmy_camels_test  &lt;- testing(my_camels_split)\n\nmy_camels_cv &lt;- vfold_cv(my_camels_train, v = 10)\n\n\n\n\n\nmy_rec &lt;-  recipe(logQmean ~ baseflow_index + soil_porosity, data = my_camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_interact(terms = ~ baseflow_index:soil_porosity) %&gt;% \n  step_naomit(all_predictors(), all_outcomes())\n\nI’m choosing these predictors because daily water discharge and the porosity of soil sound like they would have an impact on mean water discharge. Soils with greater porosity will likely have lower mean discharges, and areas with higher daily water discharges will likely have higher mean discharges.\n\n\n\n\nmy_rf &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nmy_lm &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\nmy_boost &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\n\n\n\nmy_wf &lt;- workflow_set(list(my_rec), list(my_lm, my_rf, my_boost)) %&gt;%\n  workflow_map('fit_resamples', resamples = my_camels_cv) \n\n\n\n\n\nautoplot(my_wf)\n\n\n\n\n\n\n\n\n\nrank_results(my_wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    1.04   0.0432    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.248  0.0357    10 recipe       rand…     1\n3 recipe_boost_tree Prepro… rmse    1.08   0.0465    10 recipe       boos…     2\n4 recipe_boost_tree Prepro… rsq     0.220  0.0396    10 recipe       boos…     2\n5 recipe_linear_reg Prepro… rmse    1.08   0.0411    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.179  0.0300    10 recipe       line…     3\n\n\nThe rand forest model fits this the best, as it has the lowest rmse and the highest rsq.\n\n\n\n\nmy_lm_wf &lt;- workflow() %&gt;%\n  add_recipe(my_rec) %&gt;%\n  add_model(my_lm) %&gt;%\n  fit(data = my_camels_train) \n\nsummary(extract_fit_engine(my_lm_wf))$coefficients\n\n                                  Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)                    -0.14577943 0.04968268 -2.934210 3.498276e-03\nbaseflow_index                  0.51722105 0.05090756 10.160006 3.642463e-22\nsoil_porosity                   0.07312772 0.04982638  1.467651 1.428304e-01\nbaseflow_index_x_soil_porosity -0.15324395 0.05951099 -2.575053 1.030998e-02\n\n\n\nmy_lm_data &lt;- augment(my_lm_wf, new_data = my_camels_test)\ndim(my_lm_data)\n\n[1] 168  61\n\n\n\nggplot(my_lm_data, aes(x = logQmean, y = .pred, colour = baseflow_index)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Baseflow Index\")\n\n\n\n\n\n\n\n\nTo be candid, the results are quite underwhelming. There appears to be minimal correlation between the variables, and the line of best fit does not align well with the scatterplot distribution."
  },
  {
    "objectID": "lab6.html#question-1",
    "href": "lab6.html#question-1",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "From the PDF, zero_q_freq represents the frequency of days where Q = 0 mm/day.\n\nlibrary(ggplot2)\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()"
  },
  {
    "objectID": "lab6.html#question-2",
    "href": "lab6.html#question-2",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "camels %&gt;%\n  select(aridity, p_mean, q_mean) %&gt;% \n  drop_na() %&gt;%\n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  scale_color_viridis_c() +\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\ncamels &lt;- camels %&gt;%\n  mutate(logQmean = log(q_mean))\n\n\ncamels_split &lt;- initial_split(camels, prop = 0.8)\n\ncamels_train &lt;- training(camels_split)\n\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) %&gt;% \n  step_naomit(all_predictors(), all_outcomes())\n\n\nbaked_data &lt;- prep(rec, camels_train) %&gt;% \n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\n\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.08342    0.02898  -2.879  0.00415 ** \naridity        -0.39770    0.06819  -5.832 9.51e-09 ***\np_mean          0.65025    0.06882   9.449  &lt; 2e-16 ***\naridity:p_mean  0.02389    0.01640   1.457  0.14583    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -0.08342    0.02898  -2.879  0.00415 ** \naridity          -0.39770    0.06819  -5.832 9.51e-09 ***\np_mean            0.65025    0.06882   9.449  &lt; 2e-16 ***\naridity_x_p_mean  0.02389    0.01640   1.457  0.14583    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\n\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\nlm_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n\nlm_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(lm_model) %&gt;%\n  fit(data = camels_train) \n\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                    Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)      -0.08341681 0.02897625 -2.878799 4.152841e-03\naridity          -0.39769793 0.06818909 -5.832281 9.507063e-09\np_mean            0.65025318 0.06882050  9.448539 1.088651e-19\naridity_x_p_mean  0.02389458 0.01640487  1.456555 1.458304e-01\n\nsummary(lm_base)$coefficients\n\n                  Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)    -0.08341681 0.02897625 -2.878799 4.152841e-03\naridity        -0.39769793 0.06818909 -5.832281 9.507063e-09\np_mean          0.65025318 0.06882050  9.448539 1.088651e-19\naridity:p_mean  0.02389458 0.01640487  1.456555 1.458304e-01\n\n\n\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\n\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_model) %&gt;%\n  fit(data = camels_train) \n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.594\n2 rsq     standard       0.736\n3 mae     standard       0.369\n\n\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n#rec &lt;- recipe(logQmean ~ ., data = camels_train)\n\n\n#camels_train$outcome &lt;- factor(camels_train$outcome, levels = c(\"no\", \"yes\"))\n\n\nboost_mod &lt;- boost_tree() %&gt;%\n  set_engine('xgboost') %&gt;%\n  set_mode(\"regression\")\n\nnn_model &lt;- bag_mlp() %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"regression\")\n\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, boost_mod, nn_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)"
  },
  {
    "objectID": "lab6.html#question-3",
    "href": "lab6.html#question-3",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "rank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.538  0.0273    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.791  0.0247    10 recipe       bag_…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n5 recipe_rand_fore… Prepro… rmse    0.567  0.0262    10 recipe       rand…     3\n6 recipe_rand_fore… Prepro… rsq     0.767  0.0238    10 recipe       rand…     3\n7 recipe_boost_tree Prepro… rmse    0.607  0.0283    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.740  0.0288    10 recipe       boos…     4\n\n\nIt seems as if the bagged MLP is the best bet for which model to use, as it has a lower rmse and higher rsq than the other models."
  },
  {
    "objectID": "lab6.html#data-splitting",
    "href": "lab6.html#data-splitting",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "set.seed(123)\n\nmy_camels_split &lt;- initial_split(camels, prop = 0.75)\n\nmy_camels_train &lt;- training(my_camels_split)\n\nmy_camels_test  &lt;- testing(my_camels_split)\n\nmy_camels_cv &lt;- vfold_cv(my_camels_train, v = 10)"
  },
  {
    "objectID": "lab6.html#recipe",
    "href": "lab6.html#recipe",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "my_rec &lt;-  recipe(logQmean ~ baseflow_index + soil_porosity, data = my_camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_interact(terms = ~ baseflow_index:soil_porosity) %&gt;% \n  step_naomit(all_predictors(), all_outcomes())\n\nI’m choosing these predictors because daily water discharge and the porosity of soil sound like they would have an impact on mean water discharge. Soils with greater porosity will likely have lower mean discharges, and areas with higher daily water discharges will likely have higher mean discharges."
  },
  {
    "objectID": "lab6.html#define-3-models",
    "href": "lab6.html#define-3-models",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "my_rf &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nmy_lm &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\nmy_boost &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")"
  },
  {
    "objectID": "lab6.html#workflow-set",
    "href": "lab6.html#workflow-set",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "my_wf &lt;- workflow_set(list(my_rec), list(my_lm, my_rf, my_boost)) %&gt;%\n  workflow_map('fit_resamples', resamples = my_camels_cv)"
  },
  {
    "objectID": "lab6.html#evaluation",
    "href": "lab6.html#evaluation",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "autoplot(my_wf)\n\n\n\n\n\n\n\n\n\nrank_results(my_wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    1.04   0.0432    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.248  0.0357    10 recipe       rand…     1\n3 recipe_boost_tree Prepro… rmse    1.08   0.0465    10 recipe       boos…     2\n4 recipe_boost_tree Prepro… rsq     0.220  0.0396    10 recipe       boos…     2\n5 recipe_linear_reg Prepro… rmse    1.08   0.0411    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.179  0.0300    10 recipe       line…     3\n\n\nThe rand forest model fits this the best, as it has the lowest rmse and the highest rsq."
  },
  {
    "objectID": "lab6.html#extract-and-evaluate",
    "href": "lab6.html#extract-and-evaluate",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "my_lm_wf &lt;- workflow() %&gt;%\n  add_recipe(my_rec) %&gt;%\n  add_model(my_lm) %&gt;%\n  fit(data = my_camels_train) \n\nsummary(extract_fit_engine(my_lm_wf))$coefficients\n\n                                  Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)                    -0.14577943 0.04968268 -2.934210 3.498276e-03\nbaseflow_index                  0.51722105 0.05090756 10.160006 3.642463e-22\nsoil_porosity                   0.07312772 0.04982638  1.467651 1.428304e-01\nbaseflow_index_x_soil_porosity -0.15324395 0.05951099 -2.575053 1.030998e-02\n\n\n\nmy_lm_data &lt;- augment(my_lm_wf, new_data = my_camels_test)\ndim(my_lm_data)\n\n[1] 168  61\n\n\n\nggplot(my_lm_data, aes(x = logQmean, y = .pred, colour = baseflow_index)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Baseflow Index\")\n\n\n\n\n\n\n\n\nTo be candid, the results are quite underwhelming. There appears to be minimal correlation between the variables, and the line of best fit does not align well with the scatterplot distribution."
  },
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "Lab 8 Machine Learning hyperparameter-tuning",
    "section": "",
    "text": "Introduction\nIn this lab, we will apply the concepts we learned in Unit 3 on Machine Learning, such as feature engineering, resampling, model evaluation, and hyperparameter tuning, to a regression problem. Specifically, we will predict the q_mean variable using the CAMELS dataset and create a complete machine learning pipeline.\n\n\nLab Setup\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) |&gt; \n  power_full_join(by = 'gauge_id')\n\n\n\nData Cleaning and EDA\n\ncamels_clean &lt;- camels |&gt; \n  janitor::clean_names() |&gt; \n  drop_na() \n\nskimr::skim(camels_clean)\n\n\nData summary\n\n\nName\ncamels_clean\n\n\nNumber of rows\n507\n\n\nNumber of columns\n58\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n52\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngauge_id\n0\n1\n8\n8\n0\n507\n0\n\n\nhigh_prec_timing\n0\n1\n3\n3\n0\n4\n0\n\n\nlow_prec_timing\n0\n1\n3\n3\n0\n4\n0\n\n\ngeol_1st_class\n0\n1\n12\n31\n0\n12\n0\n\n\ngeol_2nd_class\n0\n1\n12\n31\n0\n13\n0\n\n\ndom_land_cover\n0\n1\n12\n38\n0\n12\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\np_mean\n0\n1\n3.15\n1.49\n0.64\n2.20\n3.05\n3.70\n8.94\n▅▇▂▁▁\n\n\npet_mean\n0\n1\n2.80\n0.56\n1.94\n2.37\n2.70\n3.16\n4.74\n▇▇▅▂▁\n\n\np_seasonality\n0\n1\n-0.06\n0.56\n-1.44\n-0.41\n0.08\n0.27\n0.92\n▂▃▃▇▃\n\n\nfrac_snow\n0\n1\n0.19\n0.21\n0.00\n0.04\n0.11\n0.26\n0.91\n▇▂▁▁▁\n\n\naridity\n0\n1\n1.13\n0.67\n0.22\n0.73\n0.89\n1.45\n5.21\n▇▃▁▁▁\n\n\nhigh_prec_freq\n0\n1\n21.00\n4.80\n7.90\n18.45\n22.20\n24.55\n32.70\n▂▃▇▇▁\n\n\nhigh_prec_dur\n0\n1\n1.37\n0.19\n1.10\n1.22\n1.32\n1.47\n2.07\n▇▅▂▁▁\n\n\nlow_prec_freq\n0\n1\n256.92\n36.76\n169.90\n233.57\n259.15\n286.75\n348.70\n▂▅▇▆▁\n\n\nlow_prec_dur\n0\n1\n6.29\n3.46\n2.79\n4.42\n5.14\n7.21\n36.51\n▇▁▁▁▁\n\n\nglim_1st_class_frac\n0\n1\n0.73\n0.19\n0.30\n0.58\n0.76\n0.90\n1.00\n▂▅▅▆▇\n\n\nglim_2nd_class_frac\n0\n1\n0.19\n0.13\n0.00\n0.08\n0.19\n0.30\n0.49\n▇▆▆▅▂\n\n\ncarbonate_rocks_frac\n0\n1\n0.13\n0.26\n0.00\n0.00\n0.00\n0.09\n1.00\n▇▁▁▁▁\n\n\ngeol_porostiy\n0\n1\n0.12\n0.06\n0.01\n0.07\n0.12\n0.17\n0.28\n▆▆▇▆▁\n\n\ngeol_permeability\n0\n1\n-13.86\n1.13\n-16.50\n-14.66\n-13.90\n-13.03\n-10.97\n▂▃▇▅▁\n\n\nsoil_depth_pelletier\n0\n1\n9.45\n15.25\n0.27\n1.00\n1.20\n6.99\n50.00\n▇▁▁▁▁\n\n\nsoil_depth_statsgo\n0\n1\n1.28\n0.27\n0.40\n1.08\n1.42\n1.50\n1.50\n▁▁▂▂▇\n\n\nsoil_porosity\n0\n1\n0.44\n0.02\n0.37\n0.43\n0.44\n0.46\n0.59\n▁▇▂▁▁\n\n\nsoil_conductivity\n0\n1\n1.67\n1.39\n0.45\n0.89\n1.34\n1.89\n10.91\n▇▁▁▁▁\n\n\nmax_water_content\n0\n1\n0.52\n0.15\n0.09\n0.41\n0.53\n0.64\n0.88\n▁▃▆▇▁\n\n\nsand_frac\n0\n1\n35.69\n15.47\n8.18\n24.62\n34.60\n43.77\n91.16\n▅▇▅▁▁\n\n\nsilt_frac\n0\n1\n33.30\n12.82\n4.13\n23.53\n33.65\n42.76\n67.77\n▂▇▇▅▁\n\n\nclay_frac\n0\n1\n20.35\n9.81\n2.08\n13.82\n18.88\n26.58\n50.35\n▃▇▅▂▁\n\n\nwater_frac\n0\n1\n0.02\n0.15\n0.00\n0.00\n0.00\n0.00\n1.71\n▇▁▁▁▁\n\n\norganic_frac\n0\n1\n0.45\n2.97\n0.00\n0.00\n0.00\n0.00\n39.37\n▇▁▁▁▁\n\n\nother_frac\n0\n1\n10.88\n16.94\n0.00\n0.00\n2.25\n15.48\n89.87\n▇▂▁▁▁\n\n\ngauge_lat\n0\n1\n39.55\n5.21\n27.05\n36.23\n39.35\n43.96\n48.66\n▂▃▇▅▆\n\n\ngauge_lon\n0\n1\n-98.08\n16.26\n-124.39\n-112.03\n-97.04\n-83.40\n-67.94\n▇▆▇▇▅\n\n\nelev_mean\n0\n1\n842.40\n824.66\n21.75\n278.50\n492.06\n1055.30\n3457.46\n▇▂▁▁▁\n\n\nslope_mean\n0\n1\n50.12\n48.29\n0.82\n7.97\n36.17\n77.62\n255.69\n▇▃▂▁▁\n\n\narea_gages2\n0\n1\n854.89\n1829.26\n4.03\n151.12\n383.82\n855.14\n25791.04\n▇▁▁▁▁\n\n\narea_geospa_fabric\n0\n1\n870.19\n1837.52\n4.10\n164.23\n397.25\n861.21\n25817.78\n▇▁▁▁▁\n\n\nfrac_forest\n0\n1\n0.61\n0.38\n0.00\n0.20\n0.75\n0.97\n1.00\n▅▁▂▂▇\n\n\nlai_max\n0\n1\n3.00\n1.52\n0.37\n1.63\n2.75\n4.60\n5.58\n▅▇▃▅▇\n\n\nlai_diff\n0\n1\n2.27\n1.32\n0.15\n1.07\n2.06\n3.49\n4.82\n▇▇▆▃▆\n\n\ngvf_max\n0\n1\n0.70\n0.17\n0.18\n0.58\n0.75\n0.86\n0.92\n▁▂▃▃▇\n\n\ngvf_diff\n0\n1\n0.31\n0.15\n0.03\n0.18\n0.29\n0.45\n0.65\n▅▇▅▇▂\n\n\ndom_land_cover_frac\n0\n1\n0.80\n0.19\n0.31\n0.64\n0.84\n0.99\n1.00\n▁▂▃▃▇\n\n\nroot_depth_50\n0\n1\n0.18\n0.03\n0.12\n0.16\n0.18\n0.19\n0.25\n▃▅▇▂▂\n\n\nroot_depth_99\n0\n1\n1.81\n0.30\n1.50\n1.51\n1.79\n2.00\n3.10\n▇▃▂▁▁\n\n\nq_mean\n0\n1\n1.46\n1.61\n0.00\n0.49\n1.00\n1.72\n9.50\n▇▁▁▁▁\n\n\nrunoff_ratio\n0\n1\n0.38\n0.24\n0.00\n0.22\n0.34\n0.51\n1.36\n▇▇▃▁▁\n\n\nslope_fdc\n0\n1\n1.19\n0.53\n0.00\n0.81\n1.24\n1.56\n2.50\n▃▆▇▆▁\n\n\nbaseflow_index\n0\n1\n0.49\n0.17\n0.01\n0.39\n0.51\n0.60\n0.98\n▁▃▇▅▁\n\n\nstream_elas\n0\n1\n1.86\n0.80\n-0.64\n1.33\n1.69\n2.25\n6.24\n▁▇▃▁▁\n\n\nq5\n0\n1\n0.17\n0.25\n0.00\n0.01\n0.08\n0.22\n1.77\n▇▁▁▁▁\n\n\nq95\n0\n1\n4.98\n5.23\n0.00\n1.67\n3.46\n6.29\n31.23\n▇▂▁▁▁\n\n\nhigh_q_freq\n0\n1\n27.15\n30.51\n0.00\n6.92\n16.15\n38.60\n172.80\n▇▂▁▁▁\n\n\nhigh_q_dur\n0\n1\n7.62\n10.98\n0.00\n1.87\n3.24\n8.82\n92.56\n▇▁▁▁▁\n\n\nlow_q_freq\n0\n1\n111.13\n86.38\n0.00\n35.35\n101.35\n173.10\n356.80\n▇▆▅▂▁\n\n\nlow_q_dur\n0\n1\n23.38\n23.12\n0.00\n10.31\n16.92\n28.20\n209.88\n▇▁▁▁▁\n\n\nzero_q_freq\n0\n1\n0.04\n0.13\n0.00\n0.00\n0.00\n0.00\n0.97\n▇▁▁▁▁\n\n\nhfd_mean\n0\n1\n185.49\n34.66\n112.25\n162.07\n177.80\n212.10\n287.75\n▂▇▅▃▁\n\n\n\n\nvisdat::vis_dat(camels_clean)\n\n\n\n\n\n\n\n\n\n\nSplit the Data\n\nset.seed(330)\n\ncamels_split &lt;- initial_split(camels_clean, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\n\n\nFeature Engineering with Recipes\n\ncamels_recipe &lt;- recipe(q_mean ~ ., data = camels_train) |&gt; \n  step_rm(gauge_lat, gauge_lon) |&gt; \n  step_normalize(all_numeric_predictors())\n\n\n\n1 Build Resamples\n\nset.seed(330)\nfolds &lt;- vfold_cv(camels_train, v = 10)\n\n\n\n2 Build 3 Candidate Models\n\n# Linear regression model\nlm_model &lt;- linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  set_mode(\"regression\")\n\n# Decision tree model\ntree_model &lt;- decision_tree() |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"regression\")\n\n# Random forest model\nrf_model &lt;- rand_forest() |&gt; \n  set_engine(\"ranger\") |&gt; \n  set_mode(\"regression\")\n\n\n\n3 Test the models\n\nmodel_set &lt;- workflow_set(\n  preproc = list(camels_recipe),\n  models = list(\n    linear = lm_model,\n    tree   = tree_model,\n    forest = rf_model\n  )\n)\n\nmodel_results &lt;- model_set |&gt; \n  workflow_map(\"fit_resamples\", resamples = folds)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: A correlation computation is required, but the inputs are size zero or one and\n               the standard deviation cannot be computed. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x8   B: x7\nThere were issues with some computations   A: x10   B: x10\n\nautoplot(model_results)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n4 Model Selection\nMy chosen model is the random forest model because it had the lowest RMSE and MAE, and the highest R² out of the three. It seems to handle the data best and probably does well because it can pick up on complex patterns that simpler models like linear regression miss.\n\n\nModel Tuning\n\n\n1 Build a model for your chosen specification\n\n# Define the random forest model with the 'trees' parameter tuned\nrf_tune_model &lt;- rand_forest(\n  mtry = tune(),  # Tunable mtry parameter\n  min_n = tune(), # Tunable min_n parameter\n  trees = tune()  # Tunable trees parameter (added)\n) |&gt; \n  set_engine(\"ranger\") |&gt; \n  set_mode(\"regression\")\n\n\n\n2 Create a Workflow\n\nwf_tune &lt;- workflow() |&gt; \n  add_model(rf_tune_model) |&gt; \n  add_recipe(camels_recipe)\n\n\n\n3 Check The Tunable Values / Ranges\n\n# Define the hyperparameters to tune, including 'trees'\ndials &lt;- parameters(\n  mtry(range = c(2, 15)),    # Number of variables to consider at each split\n  min_n(range = c(2, 10)),   # Minimum number of data points in each node\n  trees(range = c(50, 200))  # Number of trees to use in the forest\n)\n\n# Finalize the dials object\ndials &lt;- finalize(dials)\n\n\n\n4 Define the Search Space\n\n# Assuming you are working with these hyperparameters\ndials &lt;- parameters(\n  mtry(range = c(2, 15)),\n  min_n(range = c(2, 10)),\n  trees(range = c(50, 200))\n)\n\n# Now finalize the dials object\ndials &lt;- finalize(dials)\n\n# Perform the grid search\nset.seed(330)\nmy.grid &lt;- grid_latin_hypercube(dials, size = 25)\n\nWarning: `grid_latin_hypercube()` was deprecated in dials 1.3.0.\nℹ Please use `grid_space_filling()` instead.\n\n\n\n\n5 Tune the Model\n\n# Create the Latin Hypercube grid\nset.seed(330)\nmy.grid &lt;- grid_latin_hypercube(dials, size = 25)\n\n# Perform the grid search to tune the model\nmodel_params &lt;- tune_grid(\n    wf_tune,\n    resamples = folds,\n    grid = my.grid,\n    metrics = metric_set(rmse, rsq, mae),\n    control = control_grid(save_pred = TRUE)\n)\n\n# Visualize the tuning results\nautoplot(model_params)\n\n\n\n\n\n\n\n\n\n\n6 Check the skill of the tuned model\n\ncollect_metrics(model_params) |&gt; \n  arrange(mean)\n\n# A tibble: 75 × 9\n    mtry trees min_n .metric .estimator  mean     n std_err .config             \n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n 1    15   127     7 mae     standard   0.115    10 0.0101  Preprocessor1_Model…\n 2    14   184     4 mae     standard   0.120    10 0.0123  Preprocessor1_Model…\n 3    14   152     4 mae     standard   0.120    10 0.00997 Preprocessor1_Model…\n 4    13   149     3 mae     standard   0.121    10 0.0102  Preprocessor1_Model…\n 5    13   192     4 mae     standard   0.125    10 0.0125  Preprocessor1_Model…\n 6    12   102     8 mae     standard   0.132    10 0.0127  Preprocessor1_Model…\n 7    12   134     7 mae     standard   0.133    10 0.0139  Preprocessor1_Model…\n 8    11   162     3 mae     standard   0.135    10 0.0137  Preprocessor1_Model…\n 9    11    71     6 mae     standard   0.139    10 0.0128  Preprocessor1_Model…\n10     9   132     7 mae     standard   0.147    10 0.0126  Preprocessor1_Model…\n# ℹ 65 more rows\n\nshow_best(model_params, metric = \"mae\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1    15   127     7 mae     standard   0.115    10 0.0101  Preprocessor1_Model04\n2    14   184     4 mae     standard   0.120    10 0.0123  Preprocessor1_Model25\n3    14   152     4 mae     standard   0.120    10 0.00997 Preprocessor1_Model17\n4    13   149     3 mae     standard   0.121    10 0.0102  Preprocessor1_Model18\n5    13   192     4 mae     standard   0.125    10 0.0125  Preprocessor1_Model05\n\nhp_best &lt;- select_best(model_params, metric = \"mae\")\n\n\n\n7 Finalize your model\n\nwf_final &lt;- finalize_workflow(\n  wf_tune,\n  hp_best\n)\n\n\n\nFinal Model Verification\n\nfinal_wf &lt;- finalize_workflow(wf_tune, hp_best)\n\nfinal_fit &lt;- last_fit(final_wf, split = camels_split)\n\nfinal_predictions &lt;- collect_predictions(final_fit)\n\nggplot(final_predictions, aes(x = .pred, y = q_mean)) +\n  geom_point(color = \"steelblue\", alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkorange\", linetype = \"dashed\") +\n  geom_abline(intercept = 0, slope = 1, color = \"darkgreen\", linetype = \"dotted\") +\n  labs(\n    x = \"Predicted q_mean\",\n    y = \"Actual q_mean\",\n    title = \"Predicted vs Actual q_mean\",\n    subtitle = \"Final Random Forest Model Performance\",\n    caption = \"Dashed = model fit | Dotted = perfect prediction\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Map!\n\n # Fit finalized workflow to full cleaned dataset\nfinal_model_fit &lt;- fit(final_wf, camels_clean)\n\n# Use augment() to add predictions to the original data\ncamels_pred &lt;- augment(final_model_fit, new_data = camels_clean)\n\n# Residual = (truth - prediction)^2\ncamels_pred &lt;- camels_pred |&gt;\n  mutate(residual = (q_mean - .pred)^2)\n\n# Map of predicted q_mean\nmap_pred &lt;- ggplot(camels_pred, aes(x = gauge_lon, y = gauge_lat)) +\n  geom_point(aes(color = .pred), size = 3) +\n  scale_color_viridis_c(option = \"C\", name = \"Predicted q_mean\") +\n  labs(title = \"Predicted Streamflow (q_mean)\") +\n  theme_minimal()\n\n# Map of residuals\nmap_resid &lt;- ggplot(camels_pred, aes(x = gauge_lon, y = gauge_lat)) +\n  geom_point(aes(color = residual), size = 3) +\n  scale_color_viridis_c(option = \"A\", name = \"Residual (squared)\") +\n  labs(title = \"Prediction Residuals\") +\n  theme_minimal()\n\nlibrary(patchwork)\n\nmap_pred + map_resid"
  }
]
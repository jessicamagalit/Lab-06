[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(workflows)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(tune)\nlibrary(ranger)\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(nnet)\n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n\n\nFrom the PDF, zero_q_freq represents the frequency of days where Q = 0 mm/day.\n\nlibrary(ggplot2)\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\n\n\n\ncamels %&gt;%\n  select(aridity, p_mean, q_mean) %&gt;% \n  drop_na() %&gt;%\n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  scale_color_viridis_c() +\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\ncamels &lt;- camels %&gt;%\n  mutate(logQmean = log(q_mean))\n\n\ncamels_split &lt;- initial_split(camels, prop = 0.8)\n\ncamels_train &lt;- training(camels_split)\n\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) %&gt;% \n  step_naomit(all_predictors(), all_outcomes())\n\n\nbaked_data &lt;- prep(rec, camels_train) %&gt;% \n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\n\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.08342    0.02898  -2.879  0.00415 ** \naridity        -0.39770    0.06819  -5.832 9.51e-09 ***\np_mean          0.65025    0.06882   9.449  &lt; 2e-16 ***\naridity:p_mean  0.02389    0.01640   1.457  0.14583    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -0.08342    0.02898  -2.879  0.00415 ** \naridity          -0.39770    0.06819  -5.832 9.51e-09 ***\np_mean            0.65025    0.06882   9.449  &lt; 2e-16 ***\naridity_x_p_mean  0.02389    0.01640   1.457  0.14583    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\n\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\nlm_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n\nlm_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(lm_model) %&gt;%\n  fit(data = camels_train) \n\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                    Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)      -0.08341681 0.02897625 -2.878799 4.152841e-03\naridity          -0.39769793 0.06818909 -5.832281 9.507063e-09\np_mean            0.65025318 0.06882050  9.448539 1.088651e-19\naridity_x_p_mean  0.02389458 0.01640487  1.456555 1.458304e-01\n\nsummary(lm_base)$coefficients\n\n                  Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)    -0.08341681 0.02897625 -2.878799 4.152841e-03\naridity        -0.39769793 0.06818909 -5.832281 9.507063e-09\np_mean          0.65025318 0.06882050  9.448539 1.088651e-19\naridity:p_mean  0.02389458 0.01640487  1.456555 1.458304e-01\n\n\n\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\n\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_model) %&gt;%\n  fit(data = camels_train) \n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.594\n2 rsq     standard       0.736\n3 mae     standard       0.369\n\n\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n#rec &lt;- recipe(logQmean ~ ., data = camels_train)\n\n\n#camels_train$outcome &lt;- factor(camels_train$outcome, levels = c(\"no\", \"yes\"))\n\n\nboost_mod &lt;- boost_tree() %&gt;%\n  set_engine('xgboost') %&gt;%\n  set_mode(\"regression\")\n\nnn_model &lt;- bag_mlp() %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"regression\")\n\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, boost_mod, nn_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.538  0.0273    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.791  0.0247    10 recipe       bag_…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n5 recipe_rand_fore… Prepro… rmse    0.567  0.0262    10 recipe       rand…     3\n6 recipe_rand_fore… Prepro… rsq     0.767  0.0238    10 recipe       rand…     3\n7 recipe_boost_tree Prepro… rmse    0.607  0.0283    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.740  0.0288    10 recipe       boos…     4\n\n\nIt seems as if the bagged MLP is the best bet for which model to use, as it has a lower rmse and higher rsq than the other models.\n\n\n\n\n\n\n\nset.seed(123)\n\nmy_camels_split &lt;- initial_split(camels, prop = 0.75)\n\nmy_camels_train &lt;- training(my_camels_split)\n\nmy_camels_test  &lt;- testing(my_camels_split)\n\nmy_camels_cv &lt;- vfold_cv(my_camels_train, v = 10)\n\n\n\n\n\nmy_rec &lt;-  recipe(logQmean ~ baseflow_index + soil_porosity, data = my_camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_interact(terms = ~ baseflow_index:soil_porosity) %&gt;% \n  step_naomit(all_predictors(), all_outcomes())\n\nI’m choosing these predictors because daily water discharge and the porosity of soil sound like they would have an impact on mean water discharge. Soils with greater porosity will likely have lower mean discharges, and areas with higher daily water discharges will likely have higher mean discharges.\n\n\n\n\nmy_rf &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nmy_lm &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\nmy_boost &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\n\n\n\nmy_wf &lt;- workflow_set(list(my_rec), list(my_lm, my_rf, my_boost)) %&gt;%\n  workflow_map('fit_resamples', resamples = my_camels_cv) \n\n\n\n\n\nautoplot(my_wf)\n\n\n\n\n\n\n\n\n\nrank_results(my_wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    1.04   0.0432    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.248  0.0357    10 recipe       rand…     1\n3 recipe_boost_tree Prepro… rmse    1.08   0.0465    10 recipe       boos…     2\n4 recipe_boost_tree Prepro… rsq     0.220  0.0396    10 recipe       boos…     2\n5 recipe_linear_reg Prepro… rmse    1.08   0.0411    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.179  0.0300    10 recipe       line…     3\n\n\nThe rand forest model fits this the best, as it has the lowest rmse and the highest rsq.\n\n\n\n\nmy_lm_wf &lt;- workflow() %&gt;%\n  add_recipe(my_rec) %&gt;%\n  add_model(my_lm) %&gt;%\n  fit(data = my_camels_train) \n\nsummary(extract_fit_engine(my_lm_wf))$coefficients\n\n                                  Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)                    -0.14577943 0.04968268 -2.934210 3.498276e-03\nbaseflow_index                  0.51722105 0.05090756 10.160006 3.642463e-22\nsoil_porosity                   0.07312772 0.04982638  1.467651 1.428304e-01\nbaseflow_index_x_soil_porosity -0.15324395 0.05951099 -2.575053 1.030998e-02\n\n\n\nmy_lm_data &lt;- augment(my_lm_wf, new_data = my_camels_test)\ndim(my_lm_data)\n\n[1] 168  61\n\n\n\nggplot(my_lm_data, aes(x = logQmean, y = .pred, colour = baseflow_index)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Baseflow Index\")\n\n\n\n\n\n\n\n\nTo be candid, the results are quite underwhelming. There appears to be minimal correlation between the variables, and the line of best fit does not align well with the scatterplot distribution."
  },
  {
    "objectID": "lab6.html#question-1",
    "href": "lab6.html#question-1",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "From the PDF, zero_q_freq represents the frequency of days where Q = 0 mm/day.\n\nlibrary(ggplot2)\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()"
  },
  {
    "objectID": "lab6.html#question-2",
    "href": "lab6.html#question-2",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "camels %&gt;%\n  select(aridity, p_mean, q_mean) %&gt;% \n  drop_na() %&gt;%\n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  scale_color_viridis_c() +\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\ncamels &lt;- camels %&gt;%\n  mutate(logQmean = log(q_mean))\n\n\ncamels_split &lt;- initial_split(camels, prop = 0.8)\n\ncamels_train &lt;- training(camels_split)\n\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) %&gt;% \n  step_naomit(all_predictors(), all_outcomes())\n\n\nbaked_data &lt;- prep(rec, camels_train) %&gt;% \n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\n\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.08342    0.02898  -2.879  0.00415 ** \naridity        -0.39770    0.06819  -5.832 9.51e-09 ***\np_mean          0.65025    0.06882   9.449  &lt; 2e-16 ***\naridity:p_mean  0.02389    0.01640   1.457  0.14583    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -0.08342    0.02898  -2.879  0.00415 ** \naridity          -0.39770    0.06819  -5.832 9.51e-09 ***\np_mean            0.65025    0.06882   9.449  &lt; 2e-16 ***\naridity_x_p_mean  0.02389    0.01640   1.457  0.14583    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\n\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\nlm_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n\nlm_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(lm_model) %&gt;%\n  fit(data = camels_train) \n\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                    Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)      -0.08341681 0.02897625 -2.878799 4.152841e-03\naridity          -0.39769793 0.06818909 -5.832281 9.507063e-09\np_mean            0.65025318 0.06882050  9.448539 1.088651e-19\naridity_x_p_mean  0.02389458 0.01640487  1.456555 1.458304e-01\n\nsummary(lm_base)$coefficients\n\n                  Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)    -0.08341681 0.02897625 -2.878799 4.152841e-03\naridity        -0.39769793 0.06818909 -5.832281 9.507063e-09\np_mean          0.65025318 0.06882050  9.448539 1.088651e-19\naridity:p_mean  0.02389458 0.01640487  1.456555 1.458304e-01\n\n\n\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\n\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_model) %&gt;%\n  fit(data = camels_train) \n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.594\n2 rsq     standard       0.736\n3 mae     standard       0.369\n\n\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n#rec &lt;- recipe(logQmean ~ ., data = camels_train)\n\n\n#camels_train$outcome &lt;- factor(camels_train$outcome, levels = c(\"no\", \"yes\"))\n\n\nboost_mod &lt;- boost_tree() %&gt;%\n  set_engine('xgboost') %&gt;%\n  set_mode(\"regression\")\n\nnn_model &lt;- bag_mlp() %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"regression\")\n\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, boost_mod, nn_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)"
  },
  {
    "objectID": "lab6.html#question-3",
    "href": "lab6.html#question-3",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "rank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.538  0.0273    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.791  0.0247    10 recipe       bag_…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n5 recipe_rand_fore… Prepro… rmse    0.567  0.0262    10 recipe       rand…     3\n6 recipe_rand_fore… Prepro… rsq     0.767  0.0238    10 recipe       rand…     3\n7 recipe_boost_tree Prepro… rmse    0.607  0.0283    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.740  0.0288    10 recipe       boos…     4\n\n\nIt seems as if the bagged MLP is the best bet for which model to use, as it has a lower rmse and higher rsq than the other models."
  },
  {
    "objectID": "lab6.html#data-splitting",
    "href": "lab6.html#data-splitting",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "set.seed(123)\n\nmy_camels_split &lt;- initial_split(camels, prop = 0.75)\n\nmy_camels_train &lt;- training(my_camels_split)\n\nmy_camels_test  &lt;- testing(my_camels_split)\n\nmy_camels_cv &lt;- vfold_cv(my_camels_train, v = 10)"
  },
  {
    "objectID": "lab6.html#recipe",
    "href": "lab6.html#recipe",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "my_rec &lt;-  recipe(logQmean ~ baseflow_index + soil_porosity, data = my_camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_interact(terms = ~ baseflow_index:soil_porosity) %&gt;% \n  step_naomit(all_predictors(), all_outcomes())\n\nI’m choosing these predictors because daily water discharge and the porosity of soil sound like they would have an impact on mean water discharge. Soils with greater porosity will likely have lower mean discharges, and areas with higher daily water discharges will likely have higher mean discharges."
  },
  {
    "objectID": "lab6.html#define-3-models",
    "href": "lab6.html#define-3-models",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "my_rf &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nmy_lm &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\nmy_boost &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")"
  },
  {
    "objectID": "lab6.html#workflow-set",
    "href": "lab6.html#workflow-set",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "my_wf &lt;- workflow_set(list(my_rec), list(my_lm, my_rf, my_boost)) %&gt;%\n  workflow_map('fit_resamples', resamples = my_camels_cv)"
  },
  {
    "objectID": "lab6.html#evaluation",
    "href": "lab6.html#evaluation",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "autoplot(my_wf)\n\n\n\n\n\n\n\n\n\nrank_results(my_wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    1.04   0.0432    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.248  0.0357    10 recipe       rand…     1\n3 recipe_boost_tree Prepro… rmse    1.08   0.0465    10 recipe       boos…     2\n4 recipe_boost_tree Prepro… rsq     0.220  0.0396    10 recipe       boos…     2\n5 recipe_linear_reg Prepro… rmse    1.08   0.0411    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.179  0.0300    10 recipe       line…     3\n\n\nThe rand forest model fits this the best, as it has the lowest rmse and the highest rsq."
  },
  {
    "objectID": "lab6.html#extract-and-evaluate",
    "href": "lab6.html#extract-and-evaluate",
    "title": "Lab 6 Machine Learning in Hydrology",
    "section": "",
    "text": "my_lm_wf &lt;- workflow() %&gt;%\n  add_recipe(my_rec) %&gt;%\n  add_model(my_lm) %&gt;%\n  fit(data = my_camels_train) \n\nsummary(extract_fit_engine(my_lm_wf))$coefficients\n\n                                  Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)                    -0.14577943 0.04968268 -2.934210 3.498276e-03\nbaseflow_index                  0.51722105 0.05090756 10.160006 3.642463e-22\nsoil_porosity                   0.07312772 0.04982638  1.467651 1.428304e-01\nbaseflow_index_x_soil_porosity -0.15324395 0.05951099 -2.575053 1.030998e-02\n\n\n\nmy_lm_data &lt;- augment(my_lm_wf, new_data = my_camels_test)\ndim(my_lm_data)\n\n[1] 168  61\n\n\n\nggplot(my_lm_data, aes(x = logQmean, y = .pred, colour = baseflow_index)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Baseflow Index\")\n\n\n\n\n\n\n\n\nTo be candid, the results are quite underwhelming. There appears to be minimal correlation between the variables, and the line of best fit does not align well with the scatterplot distribution."
  }
]